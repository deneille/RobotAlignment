%YAML 1.1
%TAG !u! tag:unity3d.com,2011:
--- !u!114 &11400000
MonoBehaviour:
  m_ObjectHideFlags: 0
  m_CorrespondingSourceObject: {fileID: 0}
  m_PrefabInstance: {fileID: 0}
  m_PrefabAsset: {fileID: 0}
  m_GameObject: {fileID: 0}
  m_Enabled: 1
  m_EditorHideFlags: 0
  m_Script: {fileID: 11500000, guid: 702052c42a836454988958029d37c432, type: 3}
  m_Name: Enemy3_QuizData
  m_EditorClassIdentifier: 
  questions:
  - Deep Q-Networks store Q-values in a lookup table for each state.
  - "In CoinRun, the agent misgeneralized the task by associating \u201Cgo right\u201D
    with reward instead of \u201Ccollect coin.\u201D"
  - "Reward hacking is easily avoided by telling the agent to \u201Cdo the right
    thing.\u201D"
  answers: 000100
  timeLimit: 20
  correctAnswers:
  - Deep Q-Networks uses too much state space so a deep neural network is used to
    approximate Q-values.
  - null
  - Reward hacking is not easily avoidable. The AI agent maximizes what it can reward,
    not what we want. Fixing this is very difficult.
